<form theme="dark">
  <label>Splunk Admin - Checks - Forwarders, Inputs, Deployment Server</label>
  <fieldset submitButton="false">
    <input type="time" token="splunk_issues_timerange">
      <label>Time Range</label>
      <default>
        <earliest>-7d@h</earliest>
        <latest>now</latest>
      </default>
    </input>
  </fieldset>
  <row>
    <panel>
      <title>Deployment Server - Errors</title>
      <table>
        <search>
          <query>index=_internal "ERROR Serverclass" OR "ERROR DSManager" OR ("WARN  DeploymentServer") OR CASE(" FATAL ") OR (TERM(DS_DC_Common) NOT "attributes cannot be handled by WebUI" NOT CASE("INFO")) sourcetype=splunkd (source=*splunkd.log)  
| stats count, earliest(_time) as earliest_time, latest(_time) AS latest_time, latest(_raw) as sample_raw by host 
| `cs_human_readable_time_format(earliest_time)`
| `cs_human_readable_time_format(latest_time)`
| sort - count</query>
          <earliest>$splunk_issues_timerange.earliest$</earliest>
          <latest>$splunk_issues_timerange.latest$</latest>
        </search>
        <option name="drilldown">none</option>
        <option name="refresh.display">progressbar</option>
      </table>
      <html>
        <p>This usually indicates a misconfigured serverclass.conf or a missing application from the deployment-apps directory</p>
      </html>
    </panel>
  </row>
  <row>
    <panel>
      <title>Forwarders - Splunk Universal Forwarders that are time shifting</title>
      <table>
        <search>
          <query>index=_internal sourcetype=splunkd (source=*splunkd.log) "Detected system time adjusted" OR "System time went "
| rex "by (?P&lt;timePeriod&gt;\d+)ms\.$"
| rex "by (?P&lt;timePeriodInSecs&gt;[\d\.]+) seconds$"
| eval timePeriod=if(isnotnull(timePeriodInSecs),timePeriodInSecs*1000,timePeriod)
| where timePeriod &gt; 100000 
| stats count, earliest(_time) as earliest_time, latest(_time) AS latest_time, latest(_raw) as sample_raw by host 
| `cs_human_readable_time_format(earliest_time)`
| `cs_human_readable_time_format(latest_time)`
| sort - count</query>
          <earliest>$splunk_issues_timerange.earliest$</earliest>
          <latest>$splunk_issues_timerange.latest$</latest>
        </search>
        <option name="drilldown">none</option>
        <option name="refresh.display">progressbar</option>
      </table>
      <html>
        <p>Detect universal forwarders that appear to be moving their clocks backwards/into the past or forwards, into the future. Timeshifting servers may need to be excluded from Splunk.</p>
        <p>The string "WARN TimeoutHeap - Either time adjusted forwards by, or event loop was descheduled for" looks similar but tends to relate to a poorly performing server rather than a time shift...</p>
      </html>
    </panel>
  </row>
  <row>
    <panel>
      <title>Forwarder - Data HTTP Receiver is Overwhelmed</title>
      <table>
        <search>
          <query>index=_internal "HttpListener - Can't handle request for" sourcetype=splunkd (source=*splunkd.log)
| stats count, earliest(_time) as earliest_time, latest(_time) AS latest_time, latest(_raw) as sample_raw by host 
| `cs_human_readable_time_format(earliest_time)`
| `cs_human_readable_time_format(latest_time)`
| sort - count</query>
          <earliest>$splunk_issues_timerange.earliest$</earliest>
          <latest>$splunk_issues_timerange.latest$</latest>
        </search>
        <option name="drilldown">none</option>
        <option name="refresh.display">progressbar</option>
      </table>
      <html>
        <p>Find if the HTTP listener cannot cope with the incoming load of data. Refer to https://docs.splunk.com/Documentation/Splunk/latest/Troubleshooting/HTTPthreadlimitissues for more information.</p>
      </html>
    </panel>
  </row>
  <row>
    <panel>
      <title>Forwarder - SSL Errors in the Logs</title>
      <table>
        <search>
          <query>index=_internal sourcetype=splunkd (source="*splunkd.log")
"sock_error = 10054. SSL Error = error:00000000:lib(0):func(0):reason(0)"
| stats count, earliest(_time) as earliest_time, latest(_time) AS latest_time, latest(_raw) as sample_raw by host 
| `cs_human_readable_time_format(earliest_time)`
| `cs_human_readable_time_format(latest_time)`
| sort - count</query>
          <earliest>$splunk_issues_timerange.earliest$</earliest>
          <latest>$splunk_issues_timerange.latest$</latest>
        </search>
        <option name="drilldown">none</option>
        <option name="refresh.display">progressbar</option>
      </table>
      <html>
        <p>Excessive SSL errors may relate to a bug in the universal forwarder, if the SSL errors relate to duplication this could cause a license usage issue.</p>
      </html>
    </panel>
  </row>
  <row>
    <panel>
      <title>Forwarder - Unusual number of duplication alerts</title>
      <table>
        <search>
          <query>index=_internal sourcetype=splunkd (source=*splunkd.log) "duplication"
| stats count, earliest(_time) as earliest_time, latest(_time) AS latest_time, latest(_raw) as sample_raw by host 
| where count &gt; 10
| `cs_human_readable_time_format(earliest_time)`
| `cs_human_readable_time_format(latest_time)`
| sort - count</query>
          <earliest>$splunk_issues_timerange.earliest$</earliest>
          <latest>$splunk_issues_timerange.latest$</latest>
        </search>
        <option name="drilldown">none</option>
      </table>
      <html>
        <p>The number of warnings about duplication seems unusually high (count&gt;10) and may require investigation.</p>
        <p>The duplication warnings will occur with indexer acknowledgement enabled and indexer shutdowns. Other circumstances likely require some kind of investigation, the issue may also appear if the forwarder is having trouble getting CPU time.</p>
      </html>
    </panel>
  </row>
  <row>
    <panel>
      <title>Forwarder - Data dropping duration</title>
      <table>
        <search>
          <query>index=_internal sourcetype=splunkd source=*splunkd.log "Queue for group" "has begun dropping" OR "has stopped dropping events" 
| rex "group\s+(?P&lt;group&gt;\S+).*?(?P&lt;state&gt;(stopped|begun))" 
| sort 0 _time
| streamstats current=f global=f window=1 values(state) AS prev_state, min(_time) AS start by host, group
| search state="stopped" AND prev_state="begun"
| eval duration=_time-start
| eval shorthost=replace(host, "^([^\.]+).*", "\1")
| eval combined = shorthost. "_" . group
| timechart limit=50 max(duration) AS duration by combined</query>
          <earliest>$splunk_issues_timerange.earliest$</earliest>
          <latest>$splunk_issues_timerange.latest$</latest>
        </search>
        <option name="drilldown">none</option>
      </table>
      <html>
        <p/>
      </html>
    </panel>
  </row>
  <row>
    <panel>
      <title>Forwarder - Channel churn issues</title>
      <table>
        <search>
          <query>index=_internal source=*metrics.log new_channels sourcetype=splunkd
| stats count, earliest(_time) as earliest_time, latest(_time) AS latest_time, avg(new_channels) AS avg_new_channels avg(removed_channels) AS avg_removed_channels, latest(_raw) as sample_raw by host 
| where avg_new_channels&gt;5000
| `cs_human_readable_time_format(earliest_time)`
| `cs_human_readable_time_format(latest_time)`
| sort - count</query>
          <earliest>$splunk_issues_timerange.earliest$</earliest>
          <latest>$splunk_issues_timerange.latest$</latest>
        </search>
        <option name="drilldown">none</option>
        <option name="refresh.display">progressbar</option>
      </table>
      <html>
        <p>As per https://community.splunk.com/t5/Getting-Data-In/Why-did-ingestion-slow-way-down-after-I-added-thousands-of-new/m-p/465796, having too many channels created/removed can cause issues.</p>
      </html>
    </panel>
  </row>
  <row>
    <panel>
      <title>File Monitoring - Exceeding File Descriptor</title>
      <table>
        <search>
          <query>index=_internal "TailReader - File descriptor cache is full" (source=*splunkd.log) sourcetype=splunkd
| eval message=coalesce(message,event_message)
| stats count, earliest(_time) as earliest_time, latest(_time) AS latest_time, values(message) as messages, latest(_raw) as sample_raw by host 
| `cs_human_readable_time_format(earliest_time)`
| `cs_human_readable_time_format(latest_time)`
| sort - count</query>
          <earliest>$splunk_issues_timerange.earliest$</earliest>
          <latest>$splunk_issues_timerange.latest$</latest>
        </search>
        <option name="drilldown">none</option>
        <option name="refresh.display">progressbar</option>
      </table>
      <html>
        <p>The file descriptor cache full message is a potential indicator that we are monitoring directories with many files and this might cause the forwarder to utilize extra CPU.</p>
      </html>
    </panel>
  </row>
  <row>
    <panel>
      <title>File Monitoring - Insufficient permission to read file</title>
      <table>
        <search>
          <query>index=_internal "Insufficient permissions to read file" sourcetype=splunkd (source=*splunkd.log)
| rex "\(hint: (?P&lt;hint&gt;[^\)]+)"
| stats count, earliest(_time) as earliest_time, latest(_time) AS latest_time, values(hint) AS hint, latest(_raw) as sample_raw by host, file
| rex field=file "'(?P&lt;source&gt;[^']+)'"
| table host, source, hint, count, earliest_time, latest_time, sample_raw
| `cs_human_readable_time_format(earliest_time)`
| `cs_human_readable_time_format(latest_time)`
| sort - count</query>
          <earliest>$splunk_issues_timerange.earliest$</earliest>
          <latest>$splunk_issues_timerange.latest$</latest>
        </search>
        <option name="drilldown">none</option>
        <option name="refresh.display">progressbar</option>
      </table>
      <html>
        <p>This search looks for insufficient permissions errors, the problem here is that we might have insufficient permissions to read a file but we might later obtain the correct permissions and then read the file (as permissions changes can happen *after* the file creation...this is why there is both a tstats listing all files (only done because I cannot find a nicer way to do this, map is possibly more compute intensive), and then a search for files.</p>
        <p>Insufficient permissions to read file + hint: No such file or directory when the file exists on a Splunk enterprise instance might require TAILING_SKIP_READ_CHECK = 1 in the splunk-launch.conf refer to splunk support for more info</p>
      </html>
    </panel>
  </row>
  <row>
    <panel>
      <title>File Monitoring - crcSalt or initCrcLength change may be required</title>
      <table>
        <search>
          <query>index=_internal "You may wish to use larger initCrcLen for this sourcetype" sourcetype=splunkd (source=*splunkd.log)
| rex "file=(?P&lt;file&gt;.+)\)\."
| regex file!="\.\d+$" 
| eval message=coalesce(message,event_message)
| stats count, earliest(_time) as earliest_time, latest(_time) AS latest_time, values(message) as messages, latest(_raw) as sample_raw by host, file
| `cs_human_readable_time_format(earliest_time)`
| `cs_human_readable_time_format(latest_time)`
| sort - count</query>
          <earliest>$splunk_issues_timerange.earliest$</earliest>
          <latest>$splunk_issues_timerange.latest$</latest>
        </search>
        <option name="drilldown">none</option>
        <option name="refresh.display">progressbar</option>
      </table>
      <html>
        <p>Look for issues relating to CRC salt on any files...the universal forwarder settings may need tweaking to ensure the file is read as expected, or it may be a rolled file.</p>
        <p>Attempt to exclude rolled files from the check by looking for the most common pattern (.1, .2, .10 or similar).</p>
        <p>This panel aims to find files where crcSalt=&lt;SOURCE&gt; might be required in the inputs.conf file or a tweak to the initCrcLen...</p>
      </html>
    </panel>
  </row>
  <row>
    <panel>
      <title>HEC - HEC issues</title>
      <table>
        <search>
          <query>index=_internal "ERROR HttpInputDataHandler" sourcetype=splunkd (source=*splunkd.log)
| eval event_message=coalesce(event_message,message) 
| rex field=event_message mode=sed "s/(http_input_body_size=)\d+|(totalRequestSize=)\d+/\1/" 
| rex field=event_message mode=sed "s/(channel=)[^,]+/channel=/" 
| stats count, earliest(_time) as earliest_time, latest(_time) AS latest_time, latest(_raw) as sample_raw by host, event_message 
| `cs_human_readable_time_format(earliest_time)`
| `cs_human_readable_time_format(latest_time)`
| sort - count</query>
          <earliest>$splunk_issues_timerange.earliest$</earliest>
          <latest>$splunk_issues_timerange.latest$</latest>
        </search>
        <option name="drilldown">none</option>
        <option name="refresh.display">progressbar</option>
      </table>
      <html>
        <p>Find if the HEC is throwing errors. Refer to https://docs.splunk.com/Documentation/Splunk/latest/Data/TroubleshootHTTPEventCollector for more information.</p>
      </html>
    </panel>
  </row>
  <row>
    <panel>
      <title>Output - Bandwidth Throttling Occurring</title>
      <table>
        <search>
          <query>index=_internal "has reached maxKBps. As a result, data forwarding may be throttled" sourcetype=splunkd (source=*splunkd.log)
| bin _time span=1h
| stats count, earliest(_time) as earliest_time, latest(_time) AS latest_time, latest(_raw) as sample_raw by host 
| where count &gt; 1
| `cs_human_readable_time_format(earliest_time)`
| `cs_human_readable_time_format(latest_time)`
| sort - count</query>
          <earliest>$splunk_issues_timerange.earliest$</earliest>
          <latest>$splunk_issues_timerange.latest$</latest>
        </search>
        <option name="drilldown">none</option>
        <option name="refresh.display">progressbar</option>
      </table>
      <html>
        <p>This alert detects universal or heavy forwarders that have hit the maxKbps setting in the limits.conf and might need to be investigated.</p>
      </html>
    </panel>
  </row>
  <row>
    <panel>
      <title>Output - Read operation timed out expecting ACK</title>
      <table>
        <search>
          <query>index=_internal "Read operation timed out expecting ACK from" sourcetype=splunkd (source=*splunkd.log)
| rex "from (?P&lt;indexer&gt;\S+)"
| stats count, earliest(_time) as earliest_time, latest(_time) AS latest_time, latest(_raw) as sample_raw by host, indexer
| `cs_human_readable_time_format(earliest_time)`
| `cs_human_readable_time_format(latest_time)`
| sort - count</query>
          <earliest>$splunk_issues_timerange.earliest$</earliest>
          <latest>$splunk_issues_timerange.latest$</latest>
        </search>
        <option name="drilldown">none</option>
        <option name="refresh.display">progressbar</option>
      </table>
      <html>
        <p>This read operation timed out expecting ACK will likely result in the forwarder re-sending at least some data to the indexer. This can be caused by lack of CPU on the forwarder and potentially other issues.</p>
      </html>
    </panel>
  </row>
  <row>
    <panel>
      <title>Output - Issue sending data to Indexer/HF</title>
      <table>
        <search>
          <query>index=_internal sourcetype=splunkd (source=*splunkd.log) "Could not send data to output queue"
| bin _time span=1h
| stats count, earliest(_time) as earliest_time, latest(_time) AS latest_time, latest(_raw) as sample_raw by host, _time
| where count&gt;5
| stats count, earliest(earliest_time) as earliest_time, latest(latest_time) AS latest_time, latest(sample_raw) as sample_raw by host
| `cs_human_readable_time_format(earliest_time)`
| `cs_human_readable_time_format(latest_time)`
| sort - count</query>
          <earliest>$splunk_issues_timerange.earliest$</earliest>
          <latest>$splunk_issues_timerange.latest$</latest>
        </search>
        <option name="drilldown">none</option>
        <option name="refresh.display">progressbar</option>
      </table>
      <html>
        <p>A could not send data to output queue from the indexers or heavy forwarders often indicates a performance issue.</p>
      </html>
    </panel>
  </row>
  <row>
    <panel>
      <title>Output - TCP Output Processor has paused the data flow</title>
      <table>
        <search>
          <query>index=_internal "The TCP output processor has paused the data flow. Forwarding to output group" sourcetype=splunkd (source=*splunkd.log)
| rex "has been blocked for (?P&lt;timeperiod&gt;\d+)"
| eval message=coalesce(message,event_message)
| stats count, earliest(_time) as earliest_time, latest(_time) AS latest_time, latest(message) as latest_message, max(timeperiod) as maxInSeconds, avg(timeperiod) as avgTimePeriod, latest(_raw) as sample_raw by host 
| `cs_human_readable_time_format(earliest_time)`
| `cs_human_readable_time_format(latest_time)`
| sort - count</query>
          <earliest>$splunk_issues_timerange.earliest$</earliest>
          <latest>$splunk_issues_timerange.latest$</latest>
        </search>
        <option name="drilldown">none</option>
        <option name="refresh.display">progressbar</option>
      </table>
      <html>
        <p>A paused TCP output processor is a potential indicator of an index performance issue, you may wish to ignore the shorter pause times such as 10 seconds if this is creating too many alerts.</p>
        <p>On the indexer side you may see 'WARN TcpInputProc - Stopping all listening ports. Queues blocked for more than...' OR 'WARN TcpInputProc - Started listening on tcp ports. Queues unblocked', if there are indexer performance issues.</p>
      </html>
    </panel>
  </row>
</form>